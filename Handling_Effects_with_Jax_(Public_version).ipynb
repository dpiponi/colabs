{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpiponi/colabs/blob/main/Handling_Effects_with_Jax_(Public_version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFx03bSFXqxo"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Note: this code was recently updated to support more recent versions of Jax. I know this colab runs with 0.7.2 and I believe more recent versions should work too.\n",
        "\n",
        "Here is a simple Haskell function that makes use of the list monad to implement non-determinism.\n",
        "\n",
        "```\n",
        "f x y z = do\n",
        "  u <- x\n",
        "  v <- (2 *) <$> y\n",
        "  w <- (v +) <$> z\n",
        "  return $ u * v + w\n",
        "```\n",
        "\n",
        "For those not familiar with the Haskell list monad, the code draws `u` from each element of `x` in turn, then draws `v` from each element of `y` (multiplied by 2) in turn, and then draws `w` from `v` plus each element of z in turn. At the end, all of the values of `u * v + w` are collected into a single flat list. (In fact, it's similar to a Python list comprehension, which was derived from Haskell's.)\n",
        "\n",
        "Here is a Python function that does the same:\n",
        "\n",
        "```\n",
        "def f(x, y, z):\n",
        "  u = amb(x)\n",
        "  v = 2. * amb(y)\n",
        "  w = v + amb(z)\n",
        "  return singleton(u * v + w)\n",
        "```\n",
        "\n",
        "But importantly, it runs on the GPU and draws from `u`, `v` and `w` in parallel.\n",
        "\n",
        "How can it possibly do that? It requires running the portion of code after the `u = ...` once for each value of `u`.\n",
        "This means `amb` must be affecting the control flow of the following lines.\n",
        "Haskell has monads and do-notation for this, but Python has no obvious mechanism to allow an expression to change subsequent control flow.\n",
        "\n",
        "# Jax\n",
        "Well here's the path I'm going to take:\n",
        "I'm going to use the GPU numerics compiler [Jax](https://github.com/google/jax).\n",
        "As described at its web site\n",
        "> \"At its core, JAX is an extensible system for transforming numerical functions. Here are four of primary interest: grad, jit, vmap, and pmap\"\n",
        "\n",
        "so it's perfect for this task.\n",
        "I'm going to implement something close to effect handling for it  (Ã  la [Bauer et al](https://arxiv.org/abs/1306.6316)).\n",
        "(I'll sketch what \"effect handling\" means for us below.)\n",
        "But implementing, or even just modifying compilers isn't easy.\n",
        "So instead I'm going to modify an interpreter and then use a trick simpler than the [Futamura projections](http://blog.sigfpe.com/2009/05/three-projections-of-doctor-futamura.html) to turn the interpreter into a compiler.\n",
        "\n",
        "# A sketch of how it's going to work\n",
        "Don't take the following too literally. It's really just a sketch of how the code works.\n",
        "In particular I'm using notational abuse to say that $f=g$ means that the Python functions $f$ and $g$ are extensionally equal in the sense that applying $f$ to $x$ and applying $g$ to $x$ give the same result even though $f$ and $g$ may be implemented differently and in fact may even run on different hardware.\n",
        "\n",
        "Here's the idea: suppose we have\n",
        "\n",
        "1.   a magic \"decompiler\" function $T$ that can turn a Python function into an abstract syntax tree (AST) representing the code\n",
        "2.   an interpreter $I$ that can interpret an AST, in effect turning it back into a Python function.\n",
        "3.   a compiler, $C$, that can compile an AST, except that the result runs on the GPU.\n",
        "\n",
        "\n",
        "We expect properties like this:\n",
        "\n",
        "\n",
        "$\\begin{align}\n",
        "I(T(f)) &= f \\\\\n",
        "T(I(t)) &= t \\\\\n",
        "C(T(f)) &= f \\mbox{ except $C(T(f))$ runs faster}\\\\\n",
        "\\end{align}$\n",
        "\n",
        "Note in particular that $C\\circ T$ takes Python functions and runs them fast on the GPU. I'm guessing this is the primary feature Jax users are looking for.\n",
        "\n",
        "Note that $C(T(I(T(f))))$ is a Python-to-GPU compiler.\n",
        "\n",
        "And now comes the trick: if $I'$ is another interpreter, for example one that supports effect handling, then $C(T(I'(T(f))))$ compiles $f$ for the GPU with support for effect handling.\n",
        "\n",
        "Jax is designed to work on data in the form of \"tensors\". So $T$ can't take arbitrary functions as arguments. It can only work on functions that are ultimately built from the kinds of function that are found in the `numpy` library.\n",
        "\n",
        "# The Implementation of our compiler\n",
        "Here's the code. We start with some imports.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3MK8itaUkEa"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from functools import partial\n",
        "from typing import Any, Dict\n",
        "import builtins as _builtins\n",
        "\n",
        "from jax.extend import core\n",
        "from jax.extend import linear_util as lu\n",
        "from jax.extend import source_info_util\n",
        "\n",
        "def safe_map(f, *xs):\n",
        "    if not xs:\n",
        "        return []\n",
        "    n = len(xs[0])\n",
        "    if any(len(x) != n for x in xs[1:]):\n",
        "        raise ValueError(\"safe_map: argument lengths do not match\")\n",
        "    return list(_builtins.map(f, *xs))\n",
        "\n",
        "def safe_zip(*xs):\n",
        "    if not xs:\n",
        "        return []\n",
        "    n = len(xs[0])\n",
        "    if any(len(x) != n for x in xs[1:]):\n",
        "        raise ValueError(\"safe_zip: argument lengths do not match\")\n",
        "    return list(_builtins.zip(*xs))\n",
        "\n",
        "map = safe_map\n",
        "zip = safe_zip\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukjVJ2Ls_6Q3"
      },
      "source": [
        "The AST used by Jax is called a jaxpr. Jax provides objects of a special type used to trace a function. All of the standard operations used in `numpy` are replaced by Jax versions and operations like `+` and `*` are overloaded. The special objects are provided as arguments to your function so that instead of doing any numerical work, your function builds an AST that is a trace of its execution. As a normal user you never have to explicitly see the tracer objects, this functionality is all wrapped up for you.\n",
        "\n",
        "So given a function like\n",
        "```\n",
        "def f(x):\n",
        "  y = jnp.sum(x)  # jnp is the Jax-provided version of numpy\n",
        "  return y * y + 1\n",
        "```\n",
        "the result of tracing looks something like this\n",
        "```\n",
        "{ lambda  ; a.\n",
        "  let b = reduce_sum[ axes=(0,) ] a\n",
        "      c = mul b b\n",
        "      d = add c 1\n",
        "  in (d,) }\n",
        "```\n",
        "It's a sort of assembly language for GPUs.\n",
        "\n",
        "Over at github in [core.py is an interpreter](https://github.com/google/jax/blob/master/jax/core.py) for jaxprs called `eval_jaxpr_handler`. The main thing it does is loop over the lines in a jaxpr, performing a suitable evaluation, and updating a dictionary of values with the results.\n",
        "\n",
        "I've tweaked that in two ways.\n",
        "\n",
        "\n",
        "1.   I've replaced the loop with a recursion. This allows me to split the entire interpreter into a part that evaluates things to get a value and a continuation that does something with the value.\n",
        "2.   If the primitive evaluated has a `handler` attribute then instead of simply applying the continuation to the value the handler gets to do whatever it likes with the continuation and value. This is the key feature of Bauer et al's effect handling. As far as we're concerned an effect is something you get by applying the continuation to its argument in a non-standard way.\n",
        "\n",
        "I've only changed a handful of lines from the code at github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TPWafm3UqXF"
      },
      "source": [
        "from jax.extend import core\n",
        "\n",
        "def eval_jaxpr_handler(jaxpr: core.Jaxpr, consts, *args):\n",
        "  \"\"\"A small Jaxpr interpreter supporting handlers for primitives.\n",
        "  \"\"\"\n",
        "  env: Dict[core.Var, Any] = {}\n",
        "\n",
        "  def read(v):\n",
        "    return v.val if type(v) is core.Literal else env[v]\n",
        "\n",
        "  def write(v, val):\n",
        "    env[v] = val\n",
        "\n",
        "  map(write, jaxpr.constvars, consts)\n",
        "\n",
        "  def eval_jaxpr_loop(eqns, env, invars, args):\n",
        "    env = env.copy()\n",
        "    map(write, invars, args)\n",
        "\n",
        "    if not eqns:\n",
        "      return list(map(read, jaxpr.outvars))\n",
        "\n",
        "    eqn = eqns[0]\n",
        "    in_vals = list(map(read, eqn.invars))\n",
        "\n",
        "    subfuns, bind_params = eqn.primitive.get_bind_params(eqn.params)\n",
        "\n",
        "    ctx = getattr(eqn, \"ctx\", None)\n",
        "    ctx_mgr = getattr(ctx, \"manager\", None)\n",
        "\n",
        "    def run_eqn():\n",
        "      if hasattr(eqn.primitive, 'handler'):\n",
        "        def continuation(out_args):\n",
        "          return eval_jaxpr_loop(eqns[1:], env, eqn.outvars, [out_args])\n",
        "        return [eqn.primitive.handler(continuation, *in_vals)]\n",
        "      else:\n",
        "        ans = eqn.primitive.bind(*subfuns, *in_vals, **bind_params)\n",
        "        if not eqn.primitive.multiple_results:\n",
        "          ans = [ans]\n",
        "        return eval_jaxpr_loop(eqns[1:], env, eqn.outvars, ans)\n",
        "\n",
        "    if ctx_mgr is None:\n",
        "      return run_eqn()\n",
        "    else:\n",
        "      with ctx_mgr:\n",
        "        return run_eqn()\n",
        "\n",
        "  return eval_jaxpr_loop(jaxpr.eqns, env, jaxpr.invars, args)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzTH_IJoCh2u"
      },
      "source": [
        "Now comes the implementation of an example effect. We need the functionality of Haskell's list monad. The identity for the monad is `singleton` and the bind function is `concatMap`. Jax will provide us with the `map` bit so we just need to implement `concat` which I've called `flatten` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAp1w6GWUt3u"
      },
      "source": [
        "def flatten(xs):\n",
        "  return jnp.reshape(xs, (xs.shape[0] * xs.shape[1],) + xs.shape[2:])\n",
        "\n",
        "def singleton(x):\n",
        "  return jnp.array([x])\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFboTF7cDJp0"
      },
      "source": [
        "# The implementation of `amb`\n",
        "Now I define a new primitive called `amb`. Jax provides a `Primitive` class for this. Primitives need an \"abstract evaluation\" operation to determine the size of the result of `amb`.\n",
        "\n",
        "The `handle_list` function is similar to Haskell's bind. But instead of composing `flatten` with `map` I compose it with `jax.vmap` which maps a function in *parallel*. So the continuation is applied not just to the argument of `amb`, but to each element of it in turn. (By the way, because the continuation is used repeatedly this is not a \"tame\" effect handler.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpNdN5ZtDD13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "861ac91a-9af8-4776-a153-aa61ec82d906"
      },
      "source": [
        "amb_p = core.Primitive('amb')\n",
        "\n",
        "def handle_list(f, x):\n",
        "  return flatten(jax.vmap(f)(x)[0])\n",
        "\n",
        "amb_p.handler = handle_list\n",
        "\n",
        "def amb(xs):\n",
        "  return amb_p.bind(xs)\n",
        "\n",
        "def amb_abstract_eval(xs):\n",
        "  return jax.core.ShapedArray(xs.shape[1:], xs.dtype)\n",
        "\n",
        "amb_p.def_abstract_eval(amb_abstract_eval)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.amb_abstract_eval(xs)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>amb_abstract_eval</b><br/>def amb_abstract_eval(xs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-3109738570.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEDnt455D6gO"
      },
      "source": [
        "Now I define the functions provided by Jax that play the role of $T, I, I', C$ and $T$ that I described above.\n",
        "\n",
        "One issue is that Jax needs to be able to determine types and tensor sizes statically. When we apply $T$ to a function, it needs to see the kinds of arguments you're going to use so as to make its inferences. So $T$ here is a family of functions parameterised by the arguments $f$ will be applied to. And because `jax.jit` both traces and compiles for GPU I can implement `$C\\circ T$ as a single function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKI5uzWOD5zB"
      },
      "source": [
        "def T(*xs):\n",
        "  return lambda f: jax.make_jaxpr(f)(*xs)\n",
        "\n",
        "# The usual interpreter\n",
        "def I(f):\n",
        "  # `make_jaxpr` builds a separate \"symbol table\" containing the constants\n",
        "  # needed by the jaxpr. This is why we also pass `f.literals` into\n",
        "  # `eval_jaxpr`.\n",
        "  return lambda *xs: jax.core.eval_jaxpr(f.jaxpr, f.literals, *xs)\n",
        "\n",
        "# Our special interpreter\n",
        "def I_prime(f):\n",
        "  return lambda *xs: eval_jaxpr_handler(f.jaxpr, f.literals, *xs)\n",
        "\n",
        "CT = jax.jit"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GRxyXvXEHFA"
      },
      "source": [
        "# The example, implemented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QyGiouJUyjF"
      },
      "source": [
        "xs = jnp.arange(500.)\n",
        "ys = jnp.arange(500.)\n",
        "zs = jnp.arange(500.)\n",
        "\n",
        "def f(x, y, z):\n",
        "  u = amb(x)\n",
        "  v = 2. * amb(y)\n",
        "  w = v + amb(z)\n",
        "  return singleton(u * v + w)\n",
        "\n",
        "T_ = T(xs, ys, zs)  # Specialise `T` to the particular arguments."
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHUTBFIiHJu3"
      },
      "source": [
        "First let's look at the jaxpr for the original function. We can see the new \"assembly language mnemonic\" for `amb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7z8dDVpG8Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffb3bb4-56e3-4c07-a230-e6882b81e4fd"
      },
      "source": [
        "print(T_(f))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[500]\u001b[39m b\u001b[35m:f32[500]\u001b[39m c\u001b[35m:f32[500]\u001b[39m. \u001b[34;1mlet\n",
            "    \u001b[39;22md\u001b[35m:f32[]\u001b[39m = amb a\n",
            "    e\u001b[35m:f32[]\u001b[39m = amb b\n",
            "    f\u001b[35m:f32[]\u001b[39m = mul 2.0:f32[] e\n",
            "    g\u001b[35m:f32[]\u001b[39m = amb c\n",
            "    h\u001b[35m:f32[]\u001b[39m = add f g\n",
            "    i\u001b[35m:f32[]\u001b[39m = mul d f\n",
            "    j\u001b[35m:f32[]\u001b[39m = add i h\n",
            "    k\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=()\n",
            "      shape=(1,)\n",
            "      sharding=None\n",
            "    ] j\n",
            "  \u001b[34;1min \u001b[39;22m(k,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pJflGWuNCCg"
      },
      "source": [
        "# The big moment\n",
        "\n",
        "OK, this is it. Here's where we actually run the code with the `amb` effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFx9LBpTG2aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625fc9b3-de16-47a7-9e74-b1cab8b0b86d"
      },
      "source": [
        "result = CT(I_prime(T_(f)))(xs, ys, zs)\n",
        "print(f\"result shape = {result[0].shape}\")\n",
        "print(result)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result shape = (125000000,)\n",
            "[Array([0.00000e+00, 1.00000e+00, 2.00000e+00, ..., 4.99497e+05,\n",
            "       4.99498e+05, 4.99499e+05], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u = CT(I_prime(T_(f)))(xs, ys, zs)"
      ],
      "metadata": {
        "id": "BqbdlWRbvurw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PldFvSQ-NL7m"
      },
      "source": [
        "Let's have a look at the jaxpr generated after the `amb` effect is handled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o9dCqR1U1_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c878dca-c76f-4e42-ef26-13225499207c"
      },
      "source": [
        "print(T_(I_prime(T_(f))))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[500]\u001b[39m b\u001b[35m:f32[500]\u001b[39m c\u001b[35m:f32[500]\u001b[39m. \u001b[34;1mlet\n",
            "    \u001b[39;22md\u001b[35m:f32[500]\u001b[39m = mul 2.0:f32[] b\n",
            "    e\u001b[35m:f32[1,500]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(np.int64(1),)\n",
            "      shape=(1, 500)\n",
            "      sharding=None\n",
            "    ] c\n",
            "    f\u001b[35m:f32[500,1]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(0,)\n",
            "      shape=(500, 1)\n",
            "      sharding=None\n",
            "    ] d\n",
            "    g\u001b[35m:f32[500,500]\u001b[39m = add f e\n",
            "    h\u001b[35m:f32[1,500]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(np.int64(1),)\n",
            "      shape=(1, 500)\n",
            "      sharding=None\n",
            "    ] d\n",
            "    i\u001b[35m:f32[500,1]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(0,)\n",
            "      shape=(500, 1)\n",
            "      sharding=None\n",
            "    ] a\n",
            "    j\u001b[35m:f32[500,500]\u001b[39m = mul i h\n",
            "    k\u001b[35m:f32[500,500,1]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(0, np.int64(1))\n",
            "      shape=(500, 500, 1)\n",
            "      sharding=None\n",
            "    ] j\n",
            "    l\u001b[35m:f32[1,500,500]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(np.int64(1), np.int64(2))\n",
            "      shape=(1, 500, 500)\n",
            "      sharding=None\n",
            "    ] g\n",
            "    m\u001b[35m:f32[500,500,500]\u001b[39m = add k l\n",
            "    n\u001b[35m:f32[500,500,500,1]\u001b[39m = broadcast_in_dim[\n",
            "      broadcast_dimensions=(0, np.int64(1), np.int64(2))\n",
            "      shape=(500, 500, 500, 1)\n",
            "      sharding=None\n",
            "    ] m\n",
            "    o\u001b[35m:f32[500,500,500]\u001b[39m = reshape[\n",
            "      dimensions=None\n",
            "      new_sizes=(500, 500, 500)\n",
            "      sharding=None\n",
            "    ] n\n",
            "    p\u001b[35m:f32[500,250000]\u001b[39m = reshape[\n",
            "      dimensions=None\n",
            "      new_sizes=(500, 250000)\n",
            "      sharding=None\n",
            "    ] o\n",
            "    q\u001b[35m:f32[125000000]\u001b[39m = reshape[\n",
            "      dimensions=None\n",
            "      new_sizes=(125000000,)\n",
            "      sharding=None\n",
            "    ] p\n",
            "  \u001b[34;1min \u001b[39;22m(q,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj8diR-xN1O5"
      },
      "source": [
        "The result is similar to the original except a bunch of reshapes and broadcasts have been inserted.\n",
        "\n",
        "# Discusson and caveats\n",
        "This method is very flexible. You can implement a wide array of handlers including readers, writers, state, debugging tools, [sow/reap](https://www.tensorflow.org/probability/oryx/notebooks/a_tour_of_oryx) and probability. In many ways it's more flexible than Haskell monads and you have some freedom with types that Haskell doesn't give you.\n",
        "\n",
        "On the negative side Jax needs to be able to determine all tensor sizes and types statically. So in a fragment of code like\n",
        "```\n",
        "a <- amb(...)\n",
        "b <- amb(...a...)\n",
        "```\n",
        "it's perfectly fine for the argument to the second amb to depend on the value of `a`, but its shape can't depend on the value of `a`. For similar reasons you can't implement Haskell's `guard` function because that would dynamically change the size of a tensor. (But maybe you could probably build a tensor of flags and plumb that through the code in a way that a user doesn't see it.)\n",
        "\n",
        "And another caveat is that in the interests of brevity I skipped some things. Jax can work with data more structured than pure tensors. You can work with Python objects that contain tensors, such as lists, tuples, or your own types. The extra structure is removed before the data goes to the GPU and is put back afterwards. I have ignored this.\n",
        "\n",
        "The function `amb` is named after a related function invented by John McCarthy who invented Lisp. [Here](http://www.randomhacks.net/2005/10/11/amb-operator/)'s where I first learnt about it. The version I describe above doesn't use backtracking and runs all paths.\n",
        "\n",
        "Lastly, whether the code above runs on a CPU or a GPU depends on how you have colab configured.\n",
        "\n",
        "And thanks to Matt Johnson on the Jax team for explaining all of the details of jaxprs and how to use them."
      ]
    }
  ]
}
